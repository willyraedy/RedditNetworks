{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/78/7cf94b3d0961b1a3036ba351c7fdc04170baa73d20fcb41240da214c83fd/google_cloud_storage-1.23.0-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 4.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth>=1.2.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.0.3 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.0.3)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-storage)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/9e/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098/google_resumable_media-0.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.14.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage) (0.4.7)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (40.8.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.24.1)\n",
      "Installing collected packages: google-resumable-media, google-cloud-storage\n",
      "  Found existing installation: google-resumable-media 0.4.1\n",
      "    Uninstalling google-resumable-media-0.4.1:\n",
      "      Successfully uninstalled google-resumable-media-0.4.1\n",
      "  Found existing installation: google-cloud-storage 1.22.0\n",
      "    Uninstalling google-cloud-storage-1.22.0:\n",
      "      Successfully uninstalled google-cloud-storage-1.22.0\n",
      "Successfully installed google-cloud-storage-1.23.0 google-resumable-media-0.5.0\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/Users/willyraedy/Sync/SideProjects/RedditResearch/credentials-ebeb319739c4.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=/Users/willyraedy/Sync/SideProjects/RedditResearch/credentials-ebeb319739c4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import storage\n",
    "\n",
    "# Instantiates a client\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "UPLOAD_SIZE = 10000\n",
    "IN_NETWORK_STRING = 'latinoamerica' # 'AskReddit'\n",
    "LOGGING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(param, value):\n",
    "    resp = requests.get(f'https://www.reddit.com/reddits.json?limit=100&{param}={value}&count=555', headers={'User-Agent': 'script:melis-thesis:v0.0.1 (by /u/wilburRay)'})\n",
    "    data = resp.json()\n",
    "    new_pagination_value = data['data'][param]\n",
    "    number_returned = data['data']['dist']\n",
    "    new_reddits = list(map(lambda r: r['data'], data['data']['children']))\n",
    "    return dict(new_pagination_value=new_pagination_value, new_reddits=new_reddits, number_returned=number_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(dataframe, gcp_filename):\n",
    "    temp_filename = 'tempCSVToUpload.csv'\n",
    "    dataframe.to_csv(temp_filename)\n",
    "    \n",
    "    bucket = storage_client.get_bucket('meli_thesis')\n",
    "    blob = bucket.blob(gcp_filename)\n",
    "    blob.upload_from_filename(temp_filename)\n",
    "    \n",
    "    os.remove(temp_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_network_reddits(dataframe):\n",
    "    return dataframe[dataframe['description_html'].str.contains(IN_NETWORK_STRING, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(list_of_reddits):\n",
    "    # upload raw results\n",
    "    \n",
    "    df = pd.DataFrame(list_of_reddits)\n",
    "\n",
    "    from_id = list_of_reddits[0]['id']\n",
    "    to_id = list_of_reddits[-1]['id']\n",
    "    count = len(list_of_reddits)\n",
    "    raw_gcp_filename = f'raw/{from_id}-{to_id}-reddits-{count}'\n",
    "    \n",
    "    if LOGGING:\n",
    "        print(f'raw_gcp_filename: {raw_gcp_filename}')\n",
    "        print(f'raw_head: {df.head(5)}')\n",
    "    \n",
    "    if not LOGGING:\n",
    "        upload_file(df, raw_gcp_filename)\n",
    "    \n",
    "    # filter out in network reddits\n",
    "    # upload in network reddits\n",
    "    \n",
    "    in_network = filter_network_reddits(df)\n",
    "    \n",
    "    in_network_count = in_network.shape[0]\n",
    "    network_gcp_filename = f'network/{from_id}-{to_id}-reddits-{in_network_count}'    \n",
    "    \n",
    "    if LOGGING:\n",
    "        print(f'in_network_filename: {network_gcp_filename}')\n",
    "        print(f'network_head: {in_network.head(5)}')\n",
    "\n",
    "    if not LOGGING:\n",
    "        upload_file(in_network, network_gcp_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all():\n",
    "    returned = 100\n",
    "    after = ''\n",
    "    before = ''\n",
    "    all_reddits = []\n",
    "    total_scraped = 0\n",
    "    \n",
    "#     while after != None:\n",
    "#         if LOGGING:\n",
    "#             print('###### new batch #####')\n",
    "#         try:\n",
    "#             result = fetch_page('after', after)\n",
    "#             all_reddits += list(map(lambda r: dict(id=r['id'], display_name=r['display_name'], public_description=r['public_description'], description_html=r['description_html'], subscribers=r['subscribers']), result['new_reddits']))\n",
    "#             after = result['new_pagination_value']\n",
    "#             returned = result['number_returned']\n",
    "#             total_scraped += returned\n",
    "#             time.sleep(1)\n",
    "            \n",
    "#             if LOGGING:\n",
    "#                 print(f'after: {after}')\n",
    "#                 print(pd.DataFrame(all_reddits).tail(3))\n",
    "#                 print(f'total scraped: {total_scraped}')\n",
    "\n",
    "#             if len(all_reddits) % UPLOAD_SIZE == 0:\n",
    "#                 print('***** uploading data *****')\n",
    "#                 process_chunk(all_reddits)\n",
    "#                 all_reddits = []\n",
    "#         except KeyboardInterrupt:\n",
    "#             raise\n",
    "#         except:\n",
    "#             e = sys.exc_info()[0]\n",
    "#             print(e)\n",
    "#             if LOGGING:\n",
    "#                 raise(e)\n",
    "    \n",
    "    print('Switching to Before')\n",
    "    \n",
    "    while before != None:\n",
    "        if LOGGING:\n",
    "            print('###### new batch #####')\n",
    "        try:\n",
    "            result = fetch_page('before', before)\n",
    "            all_reddits += list(map(lambda r: dict(id=r['id'], display_name=r['display_name'], public_description=r['public_description'], description_html=r['description_html'], subscribers=r['subscribers']), result['new_reddits']))\n",
    "            before = result['new_pagination_value']\n",
    "            returned = result['number_returned']\n",
    "            total_scraped += returned\n",
    "            time.sleep(1)\n",
    "\n",
    "            if LOGGING:\n",
    "                print(f'before: {before}')\n",
    "                print(pd.DataFrame(all_reddits).tail(3))\n",
    "                print(f'total scraped: {total_scraped}')\n",
    "\n",
    "            if len(all_reddits) % UPLOAD_SIZE == 0:\n",
    "                print('***** uploading data *****')\n",
    "                process_chunk(all_reddits)\n",
    "                all_reddits = []\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(e)\n",
    "            if LOGGING:\n",
    "                raise(e)\n",
    "                \n",
    "    if len(all_reddits) > 0:\n",
    "        print('processing last chunk')\n",
    "        process_chunk(all_reddits)\n",
    "    print('done')\n",
    "    return all_reddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to Before\n",
      "###### new batch #####\n",
      "before: t5_2qh3l\n",
      "                                     description_html display_name     id  \\\n",
      "97  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          CFB  2qm9d   \n",
      "98  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...     facepalm  2r5rp   \n",
      "99  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...        space  2qh87   \n",
      "\n",
      "                                   public_description  subscribers  \n",
      "97  A forum for all things college football. Prima...       671152  \n",
      "98  A subreddit for you to share the stupidity of ...      3027608  \n",
      "99  Share &amp; discuss informative content on:\\n\\...     16020450  \n",
      "total scraped: 100\n",
      "###### new batch #####\n",
      "before: None\n",
      "                                     description_html display_name     id  \\\n",
      "97  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          CFB  2qm9d   \n",
      "98  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...     facepalm  2r5rp   \n",
      "99  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...        space  2qh87   \n",
      "\n",
      "                                   public_description  subscribers  \n",
      "97  A forum for all things college football. Prima...       671152  \n",
      "98  A subreddit for you to share the stupidity of ...      3027608  \n",
      "99  Share &amp; discuss informative content on:\\n\\...     16020450  \n",
      "total scraped: 100\n",
      "processing last chunk\n",
      "raw_gcp_filename: raw/2qh3l-2qh87-reddits-100\n",
      "raw_head:                                     description_html display_name     id  \\\n",
      "0  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...         news  2qh3l   \n",
      "1  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...    AskReddit  2qh1i   \n",
      "2  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...       videos  2qh1e   \n",
      "3  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...        funny  2qh33   \n",
      "4  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...     politics  2cneq   \n",
      "\n",
      "                                  public_description  subscribers  \n",
      "0  /r/news is: real news articles, primarily but ...     19217753  \n",
      "1  r/AskReddit is the place to ask and answer tho...     25050734  \n",
      "2  The best place for video content of all kinds....     21494481  \n",
      "3  Welcome to r/Funny: reddit's largest humour de...     26995770  \n",
      "4  /r/Politics is for news and discussion about U...      5514734  \n",
      "in_network_filename: network/2qh3l-2qh87-reddits-0\n",
      "network_head: Empty DataFrame\n",
      "Columns: [description_html, display_name, id, public_description, subscribers]\n",
      "Index: []\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "all_reddits = fetch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_reddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3673, 5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex = df[df['display_name'] == 'guadalajara']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_html</th>\n",
       "      <th>display_name</th>\n",
       "      <th>id</th>\n",
       "      <th>public_description</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [description_html, display_name, id, public_description, subscribers]\n",
       "Index: []"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = mex.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description_html': {1161: '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://nm.reddit.com/r/Mexico/#nm\"&gt;MODO NOCTURNO&lt;/a&gt;\\n&lt;a href=\"http://reddit.com/r/Mexico/#dm\"&gt;NORMAL&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;h3&gt;Descripción&lt;/h3&gt;\\n\\n&lt;p&gt;Noticias, imágenes y links relacionados con México son bienvenidos, así como anuncios y preguntas dirigidas a la comunidad.&lt;/p&gt;\\n\\n&lt;p&gt;News, images and links related to Mexico are welcome, as well as announcements and questions addressed to the community.&lt;/p&gt;\\n\\n&lt;h3&gt;Atención&lt;/h3&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;NO SPAM&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;NO EDITES TÍTULOS / DON&amp;#39;T EDIT TITLES&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;NO ATAQUES PERSONALES / NO PERSONAL ATTACKS&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;NO INSULTES&lt;/strong&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Por favor lee el &lt;a href=\"http://www.reddit.com/r/mexico/wiki/index#wiki_1._reglas\"&gt;reglamento (es)&lt;/a&gt; antes de postear. Please read the &lt;a href=\"http://www.reddit.com/r/mexico/wiki/index#wiki_reddit_m.E9xico_.28english.29\"&gt;rules (en)&lt;/a&gt; before posting.&lt;/p&gt;\\n\\n&lt;h3&gt;FAQ&lt;/h3&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/mexico/comments/3hjm5v/que_chdos_es_el_smn_testimonio_de_mi_experencia/\"&gt;FAQ sobre el Servicio Militar Nacional&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/mexico/comments/3m79v2/podemos_poner_una_gu%C3%ADa_de_c%C3%B3mo_ser_freelance_en/\"&gt;¿Cómo ser freelancer en México?&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/mexico/comments/3wjh62/haq_aprend%C3%AD_que_puedes_dar_de_alta_tu_numero_en/\"&gt;¿Qué hacer para no recibir spam telefónico?&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/mexico/comments/3wew5z/el_peso_sigue_devalu%C3%A1ndose_en_picada_hoy_cotiza_a/\"&gt;Sobre la caída del peso respecto al dólar.&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;h3&gt;Subreddits&lt;/h3&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/mexico/wiki/subredditsamigos\"&gt;Da click aquí para ver la lista de subreddits amigos.&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/mexico/wiki/subredditsamigos\"&gt;Click here to access the list of related subreddits.&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;h3&gt;Diseño&lt;/h3&gt;\\n\\n&lt;p&gt;Diseño:  &lt;a href=\"/u/ElectroclassicM\"&gt;/u/ElectroclassicM&lt;/a&gt; \\nEfecto: robbit42 de &lt;a href=\"/r/europe\"&gt;r/europe&lt;/a&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;'},\n",
       " 'display_name': {1161: 'mexico'},\n",
       " 'id': {1161: '2qhv7'},\n",
       " 'public_description': {1161: 'MÉXICO'},\n",
       " 'subscribers': {1161: 181956}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
