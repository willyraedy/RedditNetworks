{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-storage in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.2.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media!=0.4.0,<0.5dev,>=0.3.1 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.0.3 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-auth>=1.2.0->google-cloud-storage) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.14.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from rsa>=3.1.4->google-auth>=1.2.0->google-cloud-storage) (0.4.7)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (40.8.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/willyraedy/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage) (2019.3.9)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/Users/willyraedy/Sync/SideProjects/RedditResearch/credentials-ebeb319739c4.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=/Users/willyraedy/Sync/SideProjects/RedditResearch/credentials-ebeb319739c4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import storage\n",
    "\n",
    "# Instantiates a client\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "UPLOAD_SIZE = 10000\n",
    "IN_NETWORK_STRING = 'latinoamerica' # 'AskReddit'\n",
    "LOGGING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(after):\n",
    "    resp = requests.get(f'https://www.reddit.com/reddits.json?limit=100&after={after}', headers={'User-Agent': 'script:melis-thesis:v0.0.1 (by /u/wilburRay)'})\n",
    "    data = resp.json()\n",
    "    new_after = data['data']['after']\n",
    "    number_returned = data['data']['dist']\n",
    "    new_reddits = list(map(lambda r: r['data'], data['data']['children']))\n",
    "    return dict(new_after=new_after, new_reddits=new_reddits, number_returned=number_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(dataframe, gcp_filename):\n",
    "    temp_filename = 'tempCSVToUpload.csv'\n",
    "    dataframe.to_csv(temp_filename)\n",
    "    \n",
    "    bucket = storage_client.get_bucket('meli_thesis')\n",
    "    blob = bucket.blob(gcp_filename)\n",
    "    blob.upload_from_filename(temp_filename)\n",
    "    \n",
    "    os.remove(temp_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_network_reddits(dataframe):\n",
    "    return dataframe[dataframe['description_html'].str.contains(IN_NETWORK_STRING, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(list_of_reddits):\n",
    "    # upload raw results\n",
    "    \n",
    "    df = pd.DataFrame(list_of_reddits)\n",
    "\n",
    "    from_id = list_of_reddits[0]['id']\n",
    "    to_id = list_of_reddits[1]['id']\n",
    "    count = len(list_of_reddits)\n",
    "    raw_gcp_filename = f'raw/{from_id}-{to_id}-reddits-{count}'\n",
    "    \n",
    "    if LOGGING:\n",
    "        print(f'raw_gcp_filename: {raw_gcp_filename}')\n",
    "        print(f'raw_head: {df.head(5)}')\n",
    "    \n",
    "    if not LOGGING:\n",
    "        upload_file(df, raw_gcp_filename)\n",
    "    \n",
    "    # filter out in network reddits\n",
    "    # upload in network reddits\n",
    "    \n",
    "    in_network = filter_network_reddits(df)\n",
    "    \n",
    "    in_network_count = in_network.shape[0]\n",
    "    network_gcp_filename = f'network/{from_id}-{to_id}-reddits-{in_network_count}'    \n",
    "    \n",
    "    if LOGGING:\n",
    "        print(f'in_network_filename: {network_gcp_filename}')\n",
    "        print(f'network_head: {in_network.head(5)}')\n",
    "\n",
    "    if not LOGGING:\n",
    "        upload_file(in_network, network_gcp_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all(after_arg = ''):\n",
    "    returned = 100\n",
    "    after = after_arg\n",
    "    all_reddits = []\n",
    "    total_scraped = 0\n",
    "    \n",
    "    while after != None:\n",
    "        if LOGGING:\n",
    "            print('###### new batch #####')\n",
    "        try:\n",
    "            result = fetch_page(after)\n",
    "            all_reddits += list(map(lambda r: dict(id=r['id'], display_name=r['display_name'], public_description=r['public_description'], description_html=r['description_html'], subscribers=r['subscribers']), result['new_reddits']))\n",
    "            after = result['new_after']\n",
    "            returned = result['number_returned']\n",
    "            total_scraped += returned\n",
    "            time.sleep(1)\n",
    "            \n",
    "            if LOGGING:\n",
    "                print(f'after: {after}')\n",
    "                print(pd.DataFrame(all_reddits).tail(3))\n",
    "                print(f'total scraped: {total_scraped}')\n",
    "\n",
    "            if len(all_reddits) % UPLOAD_SIZE == 0:\n",
    "                print('***** uploading data *****')\n",
    "                process_chunk(all_reddits)\n",
    "                all_reddits = []\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(e)\n",
    "            if LOGGING:\n",
    "                raise(e)\n",
    "                \n",
    "    if len(all_reddits) > 0:\n",
    "        print('processing last chunk')\n",
    "        process_chunk(all_reddits)\n",
    "    print('done')\n",
    "    return all_reddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing last chunk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "all_reddits = fetch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_reddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3673, 5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex = df[df['display_name'] == 'mexico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_html</th>\n",
       "      <th>display_name</th>\n",
       "      <th>id</th>\n",
       "      <th>public_description</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>mexico</td>\n",
       "      <td>2qhv7</td>\n",
       "      <td>MÉXICO</td>\n",
       "      <td>181956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       description_html display_name     id  \\\n",
       "1161  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...       mexico  2qhv7   \n",
       "\n",
       "     public_description  subscribers  \n",
       "1161             MÉXICO       181956  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
